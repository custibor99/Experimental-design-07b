{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f442cd55-a811-4a69-955e-e47c9c71d11f",
   "metadata": {},
   "source": [
    "# Notes\n",
    "- The original authors build univariate models\n",
    "    - Created one multivariate model per variable in dataset\n",
    "    - Computed mse and mae based on all of their results\n",
    "- Reference articles ceated univariate and multivariate models.\n",
    "    - The univariate models were only build with the OT variable\n",
    "- The splits assume a month is 30 days long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "594e9ce2-b455-41e9-b178-05e28d6711d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from typing import Tuple, Iterator, List\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1e386248-313c-4f3d-be48-93550d59bc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, filepath:str):\n",
    "        self.filepath = filepath\n",
    "        self.dataset_name = self.filepath.split(\"/\")[-1].split(\".\")[0]\n",
    "        self.df = self.load_data(self.filepath)\n",
    "        self.columns = self.df.columns\n",
    "    \n",
    "    def load_data(self, filepath:str) -> pd.DataFrame:\n",
    "        df = pd.read_csv(filepath)\n",
    "        df = df.drop(columns=\"date\")\n",
    "        return df\n",
    "\n",
    "    def get_train_test_data(self, df:pd.DataFrame, input_size = 672, padding=True) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        n, d = df.shape\n",
    "        breakpoint = round(n*0.8)\n",
    "        train = df.iloc[0:breakpoint]\n",
    "        test = df.iloc[breakpoint-input_size:] if padding else df.iloc[breakpoint:]\n",
    "        return train, test\n",
    "    \n",
    "    def get_lag_response_and_dependent(self, df: pd.DataFrame, input_size: int, horizon_size:int)-> pd.DataFrame:\n",
    "        n, d = df.shape\n",
    "        w = input_size\n",
    "        h = horizon_size\n",
    "        X = np.zeros([n + 1 - w - h, d*w])\n",
    "        y = np.zeros([n + 1 - w - h, d*h])\n",
    "        x_flat = df.values.flatten()\n",
    "        for i in range(0,n + 1 - w - h):\n",
    "            X[i] = x_flat[i*d:d*(i+w)]\n",
    "            y[i] = x_flat[d*(i+w):d*(i+w) + d*h]\n",
    "        return X, y\n",
    "\n",
    "    def build_dataset(self, input_size = 672, horizon_size = 24, cut_off=0.75) -> Iterator[Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]]:\n",
    "        df = self.df.copy()\n",
    "        #Build train and test dataset\n",
    "        train, test = self.get_train_test_data(df, input_size = input_size)\n",
    "\n",
    "        sc = StandardScaler()\n",
    "        cut_off = round(len(train)*cut_off)\n",
    "        self.columns = train.columns\n",
    "        sc.fit(train.iloc[0:cut_off])\n",
    "        train = pd.DataFrame(sc.transform(train))\n",
    "        test = pd.DataFrame(sc.transform(test))\n",
    "        \n",
    "        X_train, y_train = self.get_lag_response_and_dependent(train, input_size, horizon_size)\n",
    "        X_test, y_test = self.get_lag_response_and_dependent(test, input_size, horizon_size)\n",
    "\n",
    "        X_train = pd.DataFrame(X_train)\n",
    "        y_train = pd.DataFrame(y_train)\n",
    "        X_test = pd.DataFrame(X_test)\n",
    "        y_test = pd.DataFrame(y_test)\n",
    "\n",
    "        # Create hierarhical columns\n",
    "        X_cols_lvl2 = [ f\"t={i-input_size}\" for i in range(0,input_size) for _ in range(len(self.columns))]\n",
    "        X_cols_lvl1 = [col for _ in range(input_size) for col in self.columns]\n",
    "        X_cols = pd.MultiIndex.from_tuples(zip(X_cols_lvl1, X_cols_lvl2))\n",
    "        y_cols_lvl2 = [ f\"t={i}\" for i in range(0,horizon_size) for _ in range(len(self.columns))]\n",
    "        y_cols_lvl1 = [col for _ in range(horizon_size) for col in self.columns]\n",
    "        y_cols = pd.MultiIndex.from_tuples(zip(y_cols_lvl1, y_cols_lvl2))\n",
    "        X_train.columns = X_cols\n",
    "        y_train.columns = y_cols\n",
    "        X_test.columns = X_cols\n",
    "        y_test.columns = y_cols\n",
    "        return X_train, y_train, X_test, y_test\n",
    "        \n",
    "\n",
    "# loads the ECL data in cut into smaller subframes\n",
    "class ECLDataLoader:\n",
    "    def __init__(self, filepath:str, j:int, col_number_per_split:int):\n",
    "        self.filepath = filepath\n",
    "        self.dataset_name = self.filepath.split(\"/\")[-1].split(\".\")[0]\n",
    "        self.j = j\n",
    "        self.col_number_per_split = col_number_per_split\n",
    "        self.df = self.load_data(self.filepath, self.j, self.col_number_per_split)\n",
    "    \n",
    "    def load_data(self, filepath:str, j:int, col_number_per_split:int) -> pd.DataFrame:\n",
    "        df = pd.read_csv(filepath)\n",
    "        df = df.drop(columns=\"date\")\n",
    "\n",
    "        num_cols = df.shape[1]\n",
    "        # Determine the number of splits needed\n",
    "        num_splits = num_cols // col_number_per_split   # Number of splits with 50 columns\n",
    "        \n",
    "        dfs = []\n",
    "        for i in range(num_splits):\n",
    "            dfs.append(df.iloc[:, i * col_number_per_split : (i + 1) * col_number_per_split])\n",
    "        \n",
    "        dfs.append(df.iloc[:, num_splits * col_number_per_split :])\n",
    "\n",
    "        return dfs[j]\n",
    "    \n",
    "\n",
    "class ETTDataLoader(DataLoader):\n",
    "    def __init__(self, filepath:str, months=20, day_length=24):\n",
    "        self.filepath = filepath\n",
    "        self.months = months\n",
    "        self.dataset_name = self.filepath.split(\"/\")[-1].split(\".\")[0]\n",
    "        self.boundries = [0, day_length*16*30, day_length*20*30]\n",
    "        self.df = self.load_data(self.filepath)\n",
    "        self.columns = self.df.columns\n",
    "        \n",
    "    def load_data(self, filepath:str) -> pd.DataFrame:\n",
    "        # Read file\n",
    "        ett = pd.read_csv(filepath)\n",
    "        ett = ett.drop(columns=\"date\")\n",
    "        \n",
    "        #get correct rows\n",
    "        ett = ett.iloc[0:self.boundries[2]]\n",
    "        return ett\n",
    "        \n",
    "    def get_train_test_data(self, df:pd.DataFrame, input_size = 672, padding=True, day_length=20) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        tmp = df.copy()\n",
    "        train = tmp.iloc[0: self.boundries[1]]\n",
    "        test = tmp.iloc[self.boundries[1]:]\n",
    "        if padding:\n",
    "            test = pd.concat([train.iloc[-input_size:], test])\n",
    "        return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21549f5",
   "metadata": {},
   "source": [
    "# Linear Regression Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "68cbff7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_LinearRegression_Experiment(dataLoaders, horizons, input_size=672):\n",
    "    res = []\n",
    "    lr = LinearRegression()\n",
    "    dataset_names = []\n",
    "    for j, loader in enumerate(dataLoaders):\n",
    "        mse = np.zeros([1, len(horizons)])\n",
    "        mae = np.zeros([1, len(horizons)])\n",
    "        for i, h in enumerate(horizons):\n",
    "            diffs = []\n",
    "            for col in tqdm(loader.columns):\n",
    "                X_train, y_train, X_test, y_test = loader.build_dataset(horizon_size=h, input_size=input_size)\n",
    "                lr.fit(X_train[col], y_train[col])\n",
    "                y_pred = lr.predict(X_test[col])\n",
    "                diff = y_pred - y_test[col].values\n",
    "                diffs += list(diff)\n",
    "            diffs = np.array(diffs)\n",
    "            mae[0,i] = np.mean(np.abs(diffs))\n",
    "            mse[0,i] = np.mean(diffs**2)\n",
    "        res.append(mae[0])\n",
    "        res.append(mse[0])\n",
    "    \n",
    "    \n",
    "    dataset_names = [l.dataset_name for l in dataLoaders for i in range(2)]\n",
    "    metrics = ['mae', 'mse'] * len(dataLoaders)\n",
    "    ml_index = pd.MultiIndex.from_arrays([dataset_names, metrics])\n",
    "    return pd.DataFrame(res, index=ml_index, columns=horizons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9825d939-5e89-4a03-9d5b-02c4a94084b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:07<00:00,  1.11s/it]\n",
      "100%|██████████| 7/7 [00:07<00:00,  1.04s/it]\n",
      "100%|██████████| 7/7 [00:08<00:00,  1.22s/it]\n",
      "100%|██████████| 7/7 [00:11<00:00,  1.63s/it]\n",
      "100%|██████████| 7/7 [00:17<00:00,  2.56s/it]\n",
      "100%|██████████| 7/7 [00:08<00:00,  1.18s/it]\n",
      "100%|██████████| 7/7 [00:08<00:00,  1.21s/it]\n",
      "100%|██████████| 7/7 [00:10<00:00,  1.47s/it]\n",
      "100%|██████████| 7/7 [00:13<00:00,  1.89s/it]\n",
      "100%|██████████| 7/7 [00:19<00:00,  2.72s/it]\n",
      "100%|██████████| 7/7 [00:09<00:00,  1.32s/it]\n",
      "100%|██████████| 7/7 [00:09<00:00,  1.36s/it]\n",
      "100%|██████████| 7/7 [00:10<00:00,  1.45s/it]\n",
      "100%|██████████| 7/7 [00:12<00:00,  1.78s/it]\n",
      "100%|██████████| 7/7 [00:18<00:00,  2.68s/it]\n",
      "100%|██████████| 7/7 [00:00<00:00, 21.47it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 29.33it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 30.59it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 27.30it/s]\n",
      "100%|██████████| 12/12 [01:33<00:00,  7.76s/it]\n",
      "100%|██████████| 12/12 [00:54<00:00,  4.52s/it]\n",
      "100%|██████████| 12/12 [01:59<00:00,  9.99s/it]\n",
      "100%|██████████| 12/12 [02:42<00:00, 13.55s/it]\n",
      "100%|██████████| 12/12 [04:38<00:00, 23.19s/it]\n",
      "100%|██████████| 21/21 [26:05<00:00, 74.57s/it]\n",
      "100%|██████████| 21/21 [32:34<00:00, 93.05s/it]\n",
      "100%|██████████| 21/21 [40:19<00:00, 115.19s/it]\n",
      " 10%|▉         | 2/21 [05:54<56:39, 178.94s/it]"
     ]
    }
   ],
   "source": [
    "## ETTh\n",
    "etth1DataLoader = ETTDataLoader(\"../exercise2/data/ETT-small-20231205T092053Z-001/ETT-small/ETTh1.csv\")\n",
    "etth2DataLoader = ETTDataLoader(\"../exercise2/data/ETT-small-20231205T092053Z-001/ETT-small/ETTh2.csv\")\n",
    "\n",
    "dataLoaders = [etth1DataLoader, etth2DataLoader]\n",
    "horizons = [24, 48, 168, 336, 720]\n",
    "regression_etth_res = run_LinearRegression_Experiment(dataLoaders, horizons)\n",
    "regression_etth_res.to_csv(\"data/reproduced_results/etth_regression.csv\")\n",
    "\n",
    "## ETTm\n",
    "ettm1DataLoader = ETTDataLoader(\"../exercise2/data/ETT-small-20231205T092053Z-001/ETT-small/ETTm1.csv\")\n",
    "dataLoaders = [ettm1DataLoader]\n",
    "horizons = [24, 48, 96, 228, 672]\n",
    "regression_ettm_res = run_LinearRegression_Experiment(dataLoaders, horizons)\n",
    "regression_ettm_res.to_csv(\"data/reproduced_results/ettm_regression.csv\")\n",
    "\n",
    "## ILI\n",
    "iliLoader = DataLoader(\"../exercise2/data/illness-20231205T092100Z-001/illness/national_illness.csv\")\n",
    "dataLoaders = [iliLoader]\n",
    "horizons = [24, 36, 48, 60]\n",
    "regression_ili_res = run_LinearRegression_Experiment(dataLoaders, horizons, input_size=96)\n",
    "regression_ili_res.to_csv(\"data/reproduced_results/ili_regression.csv\")\n",
    "\n",
    "## WTH\n",
    "wthLoader = DataLoader(\"../exercise2/data/WTH.csv-20231205T092445Z-001/WTH.csv\")\n",
    "dataLoaders = [wthLoader]\n",
    "horizons = [24, 48, 168, 338, 720]\n",
    "regression_wth_res = run_LinearRegression_Experiment(dataLoaders, horizons)\n",
    "regression_wth_res.to_csv(\"data/reproduced_results/wth_regression.csv\")\n",
    "\n",
    "## Weather\n",
    "weatherLoader = DataLoader(\"../exercise2/data/weather-20231205T093714Z-001/weather/weather.csv\")\n",
    "dataLoaders = [weatherLoader]\n",
    "horizons = [96, 192, 336, 720]\n",
    "regression_weather_res = run_LinearRegression_Experiment(dataLoaders, horizons)\n",
    "regression_weather_res.to_csv(\"data/reproduced_results/weather_regression.csv\")\n",
    "\n",
    "# Exchange\n",
    "exchange_rate = DataLoader(\"../exercise2/data/exchange_rate-20231205T092055Z-001/exchange_rate/exchange_rate.csv\")\n",
    "dataLoaders = [exchange_rate]\n",
    "horizons = [96, 192, 336, 720]\n",
    "regression_exchange_res = run_LinearRegression_Experiment(dataLoaders, horizons, input_size=31)\n",
    "regression_exchange_res.to_csv(\"data/reproduced_results/exchange_regression.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b90806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ECL\n",
    "ECL = DataLoader(\"../exercise2/data/ECL.csv-20231205T092501Z-001/ECL.csv\")\n",
    "dataLoaders = [ECL]\n",
    "horizons = [48, 168, 336, 720, 960]\n",
    "regression_ecl_res = run_LinearRegression_Experiment(dataLoaders, horizons)\n",
    "regression_ecl_res.to_csv(\"data/reproduced_results/ecl_regression.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edd31b5",
   "metadata": {},
   "source": [
    "# SNaive Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c7a13d1b-620d-4cb1-9a92-b5b9192e516a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def snaive_predict(series, season, horizon):\n",
    "    # check if there are enough datapoints\n",
    "    if len(series) < season:\n",
    "        raise ValueError(\"Insufficient data for forecasting.\")\n",
    "    \n",
    "    last_season_observation = series[-season:]\n",
    "\n",
    "    prediction = np.tile(last_season_observation, horizon // season + 1)[:horizon]\n",
    "\n",
    "    return prediction\n",
    "\n",
    "def run_SNaive_Experiment(dataLoaders, horizons, season, train_ratio, normalisation_ratio):\n",
    "    \n",
    "    res = []\n",
    "    dataset_names = []\n",
    "\n",
    "    for j, loader in enumerate(dataLoaders):\n",
    "        df = loader.df\n",
    "        df = df.values\n",
    "\n",
    "        num_rows, num_cols = df.shape\n",
    "\n",
    "        num_train = round(num_rows * train_ratio) # round or cut or ...\n",
    "        num_test = num_rows - num_train\n",
    "\n",
    "        # normalisation -------------------------------------------------------------\n",
    "        num_normalisation = round(num_train * normalisation_ratio)\n",
    "        \n",
    "        mean = df[:num_normalisation].mean(axis=0)\n",
    "        std = df[:num_normalisation].std(axis=0)\n",
    "\n",
    "        df_normalised = (df - mean) / std\n",
    "        # ---------------------------------------------------------------------------\n",
    "        mse = np.zeros([1, len(horizons)])\n",
    "        mae = np.zeros([1, len(horizons)])\n",
    "\n",
    "        for j, horizon in enumerate(horizons):\n",
    "            diff_per_horizon = []\n",
    "\n",
    "            for col in range(num_cols):\n",
    "                diff_per_variable = []\n",
    "                for i in range(num_test - horizon + 1):    # number of tests we can do in total\n",
    "                    train_window = df_normalised[num_train + i - season:num_train + i,col]  \n",
    "                    #print(\"last season values: \", train_window)\n",
    "                    predict_window = snaive_predict(train_window, season, horizon)\n",
    "                    #print(\"predicted values: \", predict_window)\n",
    "                    test_window = df_normalised[num_train + i:num_train + i + horizon,col]  \n",
    "                    #print(\"actual values: \", test_window)\n",
    "                    \n",
    "                    diff = np.array(test_window) - np.array(predict_window)  \n",
    "                    diff_per_variable += list(diff) # diff_per_variable = [0.2, 0.5, 1.2, ..., 0.1] diff for one whole variable ~4000items for ili\n",
    "\n",
    "                diff_per_horizon += list(diff_per_variable) # diff_per_variable = [0.2, 0.5, 1.2, ..., 0.7] diff for all variables ~28000items for ili\n",
    "            mae[0,j] = np.mean(np.abs(diff_per_horizon))\n",
    "            mse[0,j] = np.mean(np.square(diff_per_horizon))\n",
    "        res.append(mae[0])\n",
    "        res.append(mse[0])\n",
    "\n",
    "    dataset_names = [l.dataset_name for l in dataLoaders for i in range(2)]\n",
    "    metrics = ['mae', 'mse'] * len(dataLoaders)      #Joseph changed this line form this: [metric for metric in (\"mae\", \"mse\") for i in range(len(dataLoaders))]\n",
    "    ml_index = pd.MultiIndex.from_arrays([dataset_names, metrics])\n",
    "    \n",
    "    return pd.DataFrame(res, index=ml_index, columns=horizons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40c3793",
   "metadata": {},
   "source": [
    "### Train ratio for ETT:\n",
    "\n",
    "\" For ETT, the data for 1-16 months is training data, and the data for 17-20 months is test data. \"  \n",
    "\n",
    "They calculated with 1month = 30days  \n",
    "ETTDataLoader only selects the first 20 months  \n",
    "Months 1-16 are train -> train_ratio = 0.8   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "64bcd7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ETTh\n",
    "snaive_etth_res_list = [] # save the results of different seasons in the same list and create the df later\n",
    "\n",
    "etth1DataLoader = ETTDataLoader(\"../exercise2/data/ETT-small-20231205T092053Z-001/ETT-small/ETTh1.csv\")\n",
    "etth2DataLoader = ETTDataLoader(\"../exercise2/data/ETT-small-20231205T092053Z-001/ETT-small/ETTh2.csv\")\n",
    "train_ratio = 0.8\n",
    "normalisation_ratio = 0.75\n",
    "\n",
    "dataLoaders = [etth1DataLoader, etth2DataLoader]\n",
    "horizons = [24, 48]\n",
    "season = 24\n",
    "snaive_etth_res_list.append(run_SNaive_Experiment(dataLoaders, horizons, season, train_ratio, normalisation_ratio))\n",
    "\n",
    "dataLoaders = [etth1DataLoader, etth2DataLoader]\n",
    "horizons = [168, 336]\n",
    "season = 168  # 24*7\n",
    "snaive_etth_res_list.append(run_SNaive_Experiment(dataLoaders, horizons, season, train_ratio, normalisation_ratio))\n",
    "\n",
    "dataLoaders = [etth1DataLoader, etth2DataLoader]\n",
    "horizons = [720]\n",
    "season = 672  # 24*28(28days=4weeks)\n",
    "snaive_etth_res_list.append(run_SNaive_Experiment(dataLoaders, horizons, season, train_ratio, normalisation_ratio))\n",
    "\n",
    "snaive_etth_res = pd.concat([snaive_etth_res_list[0], snaive_etth_res_list[1], snaive_etth_res_list[2]], axis=1)\n",
    "snaive_etth_res.to_csv(\"data/reproduced_results/etth_snaive.csv\")\n",
    "\n",
    "## ETTm\n",
    "ettm1DataLoader = ETTDataLoader(\"../exercise2/data/ETT-small-20231205T092053Z-001/ETT-small/ETTm1.csv\")\n",
    "dataLoaders = [ettm1DataLoader]\n",
    "horizons = [24, 48, 96, 228, 672]\n",
    "season = 96  # 24*4 (15min samples)\n",
    "train_ratio = 0.8\n",
    "normalisation_ratio = 0.75\n",
    "snaive_ettm_res = run_SNaive_Experiment(dataLoaders, horizons, season, train_ratio, normalisation_ratio)\n",
    "snaive_ettm_res.to_csv(\"data/reproduced_results/ettm_snaive.csv\")\n",
    "\n",
    "## ILI\n",
    "iliLoader = DataLoader(\"../exercise2/data/illness-20231205T092100Z-001/illness/national_illness.csv\")\n",
    "dataLoaders = [iliLoader]\n",
    "horizons = [24, 36, 48, 60]\n",
    "season = 52\n",
    "train_ratio = 0.8\n",
    "normalisation_ratio = 0.875\n",
    "snaive_ili_res = run_SNaive_Experiment(dataLoaders, horizons, season, train_ratio, normalisation_ratio)\n",
    "snaive_ili_res.to_csv(\"data/reproduced_results/ili_snaive.csv\")\n",
    "\n",
    "## WTH\n",
    "wthLoader = DataLoader(\"../exercise2/data/WTH.csv-20231205T092445Z-001/WTH.csv\")\n",
    "dataLoaders = [wthLoader]\n",
    "horizons = [24, 48, 168, 338, 720]\n",
    "season = 24\n",
    "train_ratio = 0.8\n",
    "normalisation_ratio = 0.875\n",
    "snaive_wth_res = run_SNaive_Experiment(dataLoaders, horizons, season, train_ratio, normalisation_ratio)\n",
    "snaive_wth_res.to_csv(\"data/reproduced_results/wth_snaive.csv\")\n",
    "\n",
    "## Weather\n",
    "weatherLoader = DataLoader(\"../exercise2/data/weather-20231205T093714Z-001/weather/weather.csv\")\n",
    "dataLoaders = [weatherLoader]\n",
    "horizons = [96, 192, 336, 720]\n",
    "season = 144    # 24*6 (sample all 10min with daily season)\n",
    "train_ratio = 0.8\n",
    "normalisation_ratio = 0.875\n",
    "snaive_weather_res = run_SNaive_Experiment(dataLoaders, horizons, season, train_ratio, normalisation_ratio)\n",
    "snaive_weather_res.to_csv(\"data/reproduced_results/weather_snaive.csv\")\n",
    "\n",
    "# Exchange\n",
    "exchange_rate = DataLoader(\"../exercise2/data/exchange_rate-20231205T092055Z-001/exchange_rate/exchange_rate.csv\")\n",
    "dataLoaders = [exchange_rate]\n",
    "horizons = [96, 192, 336, 720]\n",
    "season = 1\n",
    "train_ratio = 0.8\n",
    "normalisation_ratio = 0.875\n",
    "snaive_exchange_res = run_SNaive_Experiment(dataLoaders, horizons, season, train_ratio, normalisation_ratio)\n",
    "snaive_exchange_res.to_csv(\"data/reproduced_results/exchange_snaive.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a07198a",
   "metadata": {},
   "source": [
    "### Runtime error with ECL\n",
    "\n",
    "We can divide the dataframe into equaly large subsets, calculate their mean and then take the mean of the means of the same.\n",
    "The dataframe has 322 - date = 321 columns. When doing that we should not have a problem when calculating.  \n",
    "The divisors of 321 are 1, 3, 107, 321. Dividing by 3 would be a bit small and dividing by 107 a bit large. Therefore we make subsets of 50columns, multiply the mean of their columns by col_number_per_split/321 and add them all up:\n",
    "\n",
    "### Solution: ECL split into multiple files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "587e9d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ECL\n",
    "horizons = [48, 168, 336, 720, 960]\n",
    "season = 168    # 24*7 (sample hourly with weekly season)\n",
    "train_ratio = 0.8\n",
    "normalisation_ratio = 0.875\n",
    "snaive_ecl_res_list = []\n",
    "col_number_per_split = 50\n",
    "total_number_of_columns = 321"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eee961da",
   "metadata": {},
   "outputs": [],
   "source": [
    "ECL = ECLDataLoader(\"../exercise2/data/ECL.csv-20231205T092501Z-001/ECL.csv\", 0, col_number_per_split) #columns 0-49\n",
    "dataLoaders = [ECL]\n",
    "snaive_ecl_res_list.append(run_SNaive_Experiment(dataLoaders, horizons, season, train_ratio, normalisation_ratio).multiply(col_number_per_split/total_number_of_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "681b092f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ECL = ECLDataLoader(\"../exercise2/data/ECL.csv-20231205T092501Z-001/ECL.csv\", 1, col_number_per_split) #columns 50-99\n",
    "dataLoaders = [ECL]\n",
    "snaive_ecl_res_list.append(run_SNaive_Experiment(dataLoaders, horizons, season, train_ratio, normalisation_ratio).multiply(col_number_per_split/total_number_of_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8dce1868",
   "metadata": {},
   "outputs": [],
   "source": [
    "ECL = ECLDataLoader(\"../exercise2/data/ECL.csv-20231205T092501Z-001/ECL.csv\", 2, col_number_per_split) #columns 100-149\n",
    "dataLoaders = [ECL]\n",
    "snaive_ecl_res_list.append(run_SNaive_Experiment(dataLoaders, horizons, season, train_ratio, normalisation_ratio).multiply(col_number_per_split/total_number_of_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8057f3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "ECL = ECLDataLoader(\"../exercise2/data/ECL.csv-20231205T092501Z-001/ECL.csv\", 3, col_number_per_split) #columns 150-199\n",
    "dataLoaders = [ECL]\n",
    "snaive_ecl_res_list.append(run_SNaive_Experiment(dataLoaders, horizons, season, train_ratio, normalisation_ratio).multiply(col_number_per_split/total_number_of_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "706d6720",
   "metadata": {},
   "outputs": [],
   "source": [
    "ECL = ECLDataLoader(\"../exercise2/data/ECL.csv-20231205T092501Z-001/ECL.csv\", 4, col_number_per_split) #columns 200-249\n",
    "dataLoaders = [ECL]\n",
    "snaive_ecl_res_list.append(run_SNaive_Experiment(dataLoaders, horizons, season, train_ratio, normalisation_ratio).multiply(col_number_per_split/total_number_of_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "35b2fd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "ECL = ECLDataLoader(\"../exercise2/data/ECL.csv-20231205T092501Z-001/ECL.csv\", 5, col_number_per_split) #columns 250-299\n",
    "dataLoaders = [ECL]\n",
    "snaive_ecl_res_list.append(run_SNaive_Experiment(dataLoaders, horizons, season, train_ratio, normalisation_ratio).multiply(col_number_per_split/total_number_of_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2e4e128a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ECL = ECLDataLoader(\"../exercise2/data/ECL.csv-20231205T092501Z-001/ECL.csv\", 6, col_number_per_split) #columns 300-320\n",
    "dataLoaders = [ECL]\n",
    "snaive_ecl_res_list.append(run_SNaive_Experiment(dataLoaders, horizons, season, train_ratio, normalisation_ratio).multiply((total_number_of_columns%col_number_per_split)/total_number_of_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2f6183c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "snaive_ecl_res = None\n",
    "\n",
    "for dataframe in snaive_ecl_res_list:\n",
    "    if snaive_ecl_res is None:\n",
    "        snaive_ecl_res = dataframe.copy()\n",
    "    else:\n",
    "        snaive_ecl_res = snaive_ecl_res.add(dataframe)\n",
    "\n",
    "snaive_ecl_res.to_csv(\"data/reproduced_results/ecl_snaive.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
